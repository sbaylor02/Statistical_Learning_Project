#Load the libraries
library(tidyverse)
library(caret)
library(mlbench)
suppressMessages(library("tidyverse"))
library(reshape2)
library(readxl)
library(rpart)
library(partykit)
library(randomForest)
library(class)
library (rminer)
library(plyr)
set.seed(1876)
dat <- read_excel("Absenteeism_at_work.xls")
setwd("~/Documents/GitHub/Statistical_Learning_Project/")
set.seed(1876)
dat <- read_excel("Absenteeism_at_work.xls")
col <- c("ID", "Reason for absence", "Month of absence", "Day of the week", "Seasons", "Disciplinary failure", "Education", "Social drinker",   "Social smoker")
dat[col] <- lapply(dat[col], as.factor)
colnames(dat) <- c("ID", "Reason", "Month", "Day", "Seasons", "Transportation_expense", "Distance", "Service_time", "Age", "Work_load", "Hit_target", "Disciplinary_failure", "Education", "Children", "Social_drinker", "Social_smoker", "Pet", "Weight", "Height", "BMI", "Absent_time")
nums <- unlist(lapply(dat, is.numeric))
dat.num <- dat[ , nums]
#change variable represent missed time one day or greater
dat <- dat %>% mutate(Absent_time= ifelse(dat$Absent_time <=8,0,1))
str(dat)
dat$Absent_time <- as.factor(dat$Absent_time)
#Transforming to Data Frame
dat <- as.data.frame(dat)
str(dat)
#scaling the data:
dat_v <- dat #we are going to use dat_v for the manipulation
scale <- sapply(dat_v, is.numeric)
dat_v[scale] <- lapply(dat_v[scale],scale)
head(dat_v)
#predicting class:
AB_class <- dat_v[, 21]
names(AB_class) <- c(1:nrow(dat_v))
dat_v$ID <- c(1:nrow(dat_v))
dat_v <- dat_v[1:737,]
nrow(dat_v)
rand_permute <- sample(x = nrow(dat_v), size = nrow(dat_v))
all_id_random <- dat_v[rand_permute, "ID"]
dat_v <- dat_v[,-1] #remove ID
#random samples for training test
validate_id <- as.character(all_id_random[1:248])
training_id <- as.character(all_id_random[249:737])
dat_v_train <- dat_v[training_id, ]
dat_v_val <- dat_v[validate_id, ]
AB_class_train <- AB_class[training_id]
AB_class_val <- AB_class[validate_id]
table(AB_class_train)
#Study significance of the variables
p <- .6 # proportion of data for training
w <- sample(1:nrow(dat_v), nrow(dat_v)*p, replace=F)
data_train <-dat_v[w,]
data_test <- dat_v[-w,]
rf <- randomForest(Absent_time ~.,
data=data_train,
mtry=6,
ntree=50,
na.action=na.roughfix)
impfact <- importance(rf)
impfact <- as.list(impfact)
names(impfact) <- colnames(dat_v[,-20])
impfact2 <- unlist(impfact)
most_sig_stats <- names(sort(desc(impfact2)))
dat_v_train_ord <- dat_v_train[ c(most_sig_stats)]
str(dat_v_train_ord)
dat_v_val_ord <- dat_v_val[, names(dat_v_train_ord)]
str(dat_v_val_ord)
size <- length(training_id)
(2/3) * length(training_id)
training_family_L <- lapply(1:500, function(j) {
perm <- sample(1:size, size = size, replace = F)
shuffle <- training_id[perm]
trn <- shuffle[1:326]
trn
})
validation_family_L <- lapply(training_family_L,
function(x) setdiff(training_id, x))
N <- seq(from = 2, to = 19, by = 1)
sqrt(length(training_family_L[[1]]))
K <- seq(from = 1, to = 15, by = 1)
times <- 500 * length(N) * length(K)
paramter_errors_df <- data.frame(mc_index = as.integer(rep(NA, times = times)),
var_num = as.integer(rep(NA, times = times)),
k = as.integer(rep(NA, times = times)),
error = as.numeric(rep(NA, times = times)))
#Core knn_model:
# j = index, n = length of range of variables, k=k
core_knn <- function(j, n, k) {
knn_predict <- knn(train = dat_v_train_ord[training_family_L[[j]], 1:n],
test = dat_v_train_ord[validation_family_L[[j]], 1:n],
cl = AB_class_train[training_family_L[[j]]],
k = k)
tbl <- table(knn_predict, AB_class_train[validation_family_L[[j]]])
err <- (tbl[1, 2] + tbl[2, 1])/(tbl[1, 2] + tbl[2, 1]+tbl[1, 1] + tbl[2, 2])
err
}
param_df1 <- merge(data.frame(mc_index = 1:500), data.frame(var_num = N))
param_df <- merge(param_df1, data.frame(k = K))
knn_err_est_df <- ddply(param_df[1:times, ], .(mc_index, var_num, k), function(df) {
err <- core_knn(df$mc_index[1], df$var_num[1], df$k[1])
err
})
head(knn_err_est_df)
knn_err_est_df <- ddply(param_df[1:times, ], .(mc_index, var_num, k), function(df) {
err <- core_knn(df$mc_index[1], df$var_num[1], df$k[1])
err
})
head(knn_err_est_df)
names(knn_err_est_df)[4] <- "error"
mean_errs_df <- ddply(knn_err_est_df, .(var_num, k), function(df) mean(df$error))
head(mean_errs_df)
names(mean_errs_df)[3] <- "mean_error"
library(ggplot2)
ggplot(data = mean_errs_df, aes(x = var_num, y = k, color = mean_error)) + geom_point(size = 5) +
theme_bw()
#This is the model that produces the lowest mean error var_num = 6 and k = 1:
mean_errs_df[which.min(mean_errs_df$mean_error), ]
mean_errs_df %>% arrange(mean_error)
N <- seq(from = 2, to = 19, by = 1)
sqrt(length(training_family_L[[1]]))
K <- seq(from = 1, to = 5, by = 1)
times <- 500 * length(N) * length(K)
core_knn_sen <- function(j, n, k) {
knn_predict <- knn(train = dat_v_train_ord[training_family_L[[j]], 1:n],
test = dat_v_train_ord[validation_family_L[[j]], 1:n],
cl = AB_class_train[training_family_L[[j]]],
k = k)
tbl <- table(knn_predict, AB_class_train[validation_family_L[[j]]])
#generate confusion matrix ( the 1 tells the model we care about that output)
cm_KNN <-  confusionMatrix(data = tbl, reference =AB_class_train[validation_family_L[[j]]], positive = "1")
sen <- cm_KNN$byClass[1]
sen
}
param_df1_2 <- merge(data.frame(mc_index = 1:500), data.frame(var_num = N))
param_df_2 <- merge(param_df1_2, data.frame(k = K))
knn_err_est_df_2 <- ddply(param_df_2[1:times, ], .(mc_index, var_num, k), function(df) {
sen <- core_knn_sen(df$mc_index[1], df$var_num[1], df$k[1])
sen
})
knn_err_est_df_2 <- ddply(param_df_2[1:times, ], .(mc_index, var_num, k), function(df) {
sen <- core_knn_sen(df$mc_index[1], df$var_num[1], df$k[1])
sen
})
head(knn_err_est_df_2)
names(knn_err_est_df_2)[4] <- "Sensitivity"
mean_sens_df <- ddply(knn_err_est_df_2, .(var_num, k), function(df) mean(df$Sensitivity))
head(mean_sens_df)
names(mean_sens_df)[3] <- "mean_sensitivity"
library(ggplot2)
ggplot(data = mean_sens_df, aes(x = var_num, y = k, color = mean_sensitivity)) + geom_point(size = 5) +
theme_bw()
ggplot(data = mean_sens_df, aes(x = var_num, y = k, color = mean_sensitivity)) + geom_point(size = 5) +
theme_minimal()
#This is the model that produces the lowest mean error var_num = 9 and k = 1:
mean_sens_df[which.max(mean_sens_df$mean_sensitivity), ]
mean_sens_df %>% arrange(desc(mean_sensitivity))
KNN_10_1 <- knn(train = dat_v_train_ord[, 1:9],
dat_v_val_ord[, 1:9], AB_class_train,
k = 1)
tbl_bm_val <- table(KNN_10_1, AB_class_val)
tbl_bm_val
cm_KNN_opt <-  confusionMatrix(data = tbl_bm_val, reference = dat_v_val_ord[, 1:6], positive = "1")
R <- 50 # replications
# create the matrix to store values 1 row per model
err_matrix_opt <- matrix(0, ncol=2, nrow=R)
sensitivity_matrix_opt <- matrix(0, ncol=2, nrow=R)
fmeasure_matrix_opt <- matrix(0, ncol=2, nrow=R)
gmean_matrix_opt <- matrix(0, ncol=2, nrow=R)
# these are optional but I like to see how the model did each run so I can check other output
KNNcm <- matrix(0, ncol=4, nrow=R)
dat_smaller <- dat[, names(dat_v_train_ord)]
dat_smaller[,20] <- dat$Absent_time
dat_smaller <- dat_smaller[1:737,] # remove lines with non-meaningful data
scale <- sapply(dat_smaller, is.numeric)
dat_smaller[scale] <- lapply(dat_smaller[scale],scale)
head(dat_smaller)
set.seed(1876)
for (r in 1:R){
# subsetting data to training and testing data
p <- .6 # proportion of data for training
w <- sample(1:nrow(dat_smaller), nrow(dat_smaller)*p, replace=F)
data_train <-dat_smaller[w,]
data_test <- dat_smaller[-w,]
################################################################ knn
#Running the classifier
knn <- knn(data_train[,1:9],
test = data_test[,1:9],
cl=data_train[,20], k=1)
#predict doesn't work with KNN for factors
knntable <- table(knn, data_test[,20])
#generate confusion matrix ( the 1 tells the model we care about that output)
cm_KNN <-  confusionMatrix(data = knntable, reference = data_test[,1:2], positive = "1")
KNNcm [[r,1]] <-  cm_KNN$table[1,1]
KNNcm [[r,2]] <-  cm_KNN$table[1,2]
KNNcm [[r,3]] <-  cm_KNN$table[2,1]
KNNcm [[r,4]] <-  cm_KNN$table[2,2]
err_matrix_opt [[r,1]] <-  (cm_KNN$table[1,2]+cm_KNN$table[2,1])/nrow( data_test)
# store the errors (change the 1 to whichever model you have)
sensitivity_matrix_opt[[r, 1]] <- cm_KNN$byClass[1]
fmeasure_matrix_opt [[r, 1]] <- cm_KNN$byClass[7]
gmean_matrix_opt [[r, 1]] <- sqrt(cm_KNN$byClass[1]* cm_KNN$byClass[2])
cat("Finished Rep",r, "\n")
}
colnames(sensitivity_matrix_opt)<- c("KNN", "other")
graph_sens <- melt(sensitivity_matrix_opt)
graph <- ggplot(graph_sens,aes(x=Var2, y=value) )+ geom_boxplot()
graph
graph <- ggplot(graph_sens,aes(x=Var2, y=value) )+ geom_boxplot() + theme_minimal()
graph
colnames(err_matrix_opt)<- c("KNN", "other")
graph_err <- melt(err_matrix_opt)
graph <- ggplot(graph_err,aes(x=Var2, y=value) )+ geom_boxplot()
graph
graph <- ggplot(graph_sens,aes(x=Var2, y=value) )+ geom_boxplot() + theme_minimal()
graph
graph <- ggplot(graph_err,aes(x=Var2, y=value) )+ geom_boxplot() + theme_minimal()
graph
graph1 <- ggplot(graph_sens,aes(x=Var2, y=value) )+ geom_boxplot() + theme_minimal()
graph1
ggplot(data = mean_sens_df, aes(x = var_num, y = k, color = mean_sensitivity)) + geom_point(size = 5) +
theme_minimal()
#This is the model that produces the lowest mean error var_num = 9 and k = 1:
mean_sens_df[which.max(mean_sens_df$mean_sensitivity), ]
mean_sens_df %>% arrange(desc(mean_sensitivity))
times <- 500 * length(N) * length(K)
times
table(dat$ID)
#frequency table by ID
dat %>% count(ID)
#frequency table by ID
dat %>% count(ID)
#Read in the data
dat <- read_excel("Absenteeism_at_work.xls")
#View the data
glimpse(dat)
#Set factored variables as factors
col <- c("ID", "Reason for absence", "Month of absence", "Day of the week", "Seasons", "Disciplinary failure", "Education", "Social drinker",   "Social smoker")
#set all categorical variables as ordered factors
dat[col] <- lapply(dat[col], as.factor)
dat[col] <- lapply(dat[col], ordered)
#Rename the columns for easier use
colnames(dat) <- c("ID", "Reason", "Month", "Day", "Seasons", "Transportation_expense", "Distance", "Service_time", "Age", "Work_load", "Hit_target", "Disciplinary_failure", "Education", "Children", "Social_drinker", "Social_smoker", "Pet", "Weight", "Height", "BMI", "Absent_time")
#View the data
glimpse(dat)
#change variable represent missed time one day or greater
dat <- dat %>% mutate(Absent_time= ifelse(dat$Absent_time <=8,0,1))
#save Absent_time as a factor in the data set
dat$Absent_time <- as.factor(dat$Absent_time)
#Transforming to Data Frame
dat <- as.data.frame(dat)
#plot the Absent_time
ggplot(data = dat,
aes(x = Absent_time)) +
geom_bar() +
theme_minimal()
#plot all variables vs. Absent_time
dat %>%
gather(-Absent_time, key = "var_name", value = "value") %>%
ggplot(aes(x = value, y = Absent_time)) +
geom_point() +
facet_wrap(~ var_name, scales = "free")
#frequency table by ID
dat %>% count(ID)
#frequency table by ID
dat %>% count(ID)
#load the necessary packages
library(plyr)
library(tidyverse)
library(reshape2)
library(readxl)
library(caret)
library(rpart)
library(partykit)
library(randomForest)
library(class)
library (rminer)
library(e1071)
library(mlbench)
library(ggplot2)
#load the necessary packages
library(plyr)
library(tidyverse)
library(reshape2)
library(readxl)
library(caret)
library(rpart)
library(partykit)
library(randomForest)
library(class)
library (rminer)
library(e1071)
library(mlbench)
library(ggplot2)
#Read in the data
dat <- read_excel("Absenteeism_at_work.xls")
#View the data
glimpse(dat)
#Set factored variables as factors
col <- c("ID", "Reason for absence", "Month of absence", "Day of the week", "Seasons", "Disciplinary failure", "Education", "Social drinker",   "Social smoker")
#set all categorical variables as ordered factors
dat[col] <- lapply(dat[col], as.factor)
dat[col] <- lapply(dat[col], ordered)
#Rename the columns for easier use
colnames(dat) <- c("ID", "Reason", "Month", "Day", "Seasons", "Transportation_expense", "Distance", "Service_time", "Age", "Work_load", "Hit_target", "Disciplinary_failure", "Education", "Children", "Social_drinker", "Social_smoker", "Pet", "Weight", "Height", "BMI", "Absent_time")
#View the data
glimpse(dat)
#create a list of the numeric variables in the data set
nums <- unlist(lapply(dat, is.numeric))
#create a smaller data set of just numeric variables
dat.num <- dat[ , nums]
summary(dat$Absent_time)
table(dat$Absent_time)
#change variable represent missed time one day or greater
dat <- dat %>% mutate(Absent_time= ifelse(dat$Absent_time <=8,0,1))
#save Absent_time as a factor in the data set
dat$Absent_time <- as.factor(dat$Absent_time)
#Transforming to Data Frame
dat <- as.data.frame(dat)
#plot the Absent_time
ggplot(data = dat,
aes(x = Absent_time)) +
geom_bar() +
theme_minimal()
#plot all variables vs. Absent_time
dat %>%
gather(-Absent_time, key = "var_name", value = "value") %>%
ggplot(aes(x = value, y = Absent_time)) +
geom_point() +
facet_wrap(~ var_name, scales = "free")
#frequency table by ID
dat %>% count(ID)
#bar chart
dat %>%
ggplot(aes(x=ID)) +
geom_bar() +
theme_minimal()
dat %>% count(Absent_time)
#plot all variables vs. Absent_time
dat %>%
gather(-Absent_time, key = "var_name", value = "value") %>%
ggplot(aes(x = value, y = Absent_time)) +
geom_point() +
facet_wrap(~ var_name, scales = "free") +
theme_mi
#plot all variables vs. Absent_time
dat %>%
gather(-Absent_time, key = "var_name", value = "value") %>%
ggplot(aes(x = value, y = Absent_time)) +
geom_point() +
facet_wrap(~ var_name, scales = "free") +
theme_minimal()
dat1 <- dat[-1]
####################Someone confirm I did this right. I scaled only numeric not factors. Thats correct right?
#scale
scale <- sapply(dat1, is.numeric)
dat1[scale] <- lapply(dat1[scale],scale)
R <- 50 # replications
# create the matrix to store values 1 row per model
err_matrix <- matrix(0, ncol=5, nrow=R)
sensitivity_matrix <- matrix(0, ncol=5, nrow=R)
fmeasure_matrix <- matrix(0, ncol=5, nrow=R)
gmean_matrix <- matrix(0, ncol=5, nrow=R)
# these are optional but I like to see how the model did each run so I can check other output
KNNcm <- matrix(0, ncol=4, nrow=R)
glmcm <- matrix(0, ncol=4, nrow=R)
Treecm <- matrix(0, ncol=4, nrow=R)
rfcm <- matrix(0, ncol=4, nrow=R)
SVMcm <- matrix(0, ncol=4, nrow=R)
set.seed(1876)
for (r in 1:R){
# subsetting data to training and testing data
p <- .6 # proportion of data for training
w <- sample(1:nrow(dat1), nrow(dat1)*p, replace=F)
data_train <-dat1[w,]
data_test <- dat1[-w,]
################################################################ knn
#Running the classifier
knn <- knn(data_train[-20],
test = data_test[-20],
cl=data_train$Absent_time, k=2)
#predict doesn't work with KNN for factors
knntable <- table(knn, data_test$Absent_time)
#generate confusion matrix ( the 1 tells the model we care about that output)
cm_KNN <-  confusionMatrix(data = knntable, reference = data_test[,-20], positive = "1")
KNNcm [[r,1]] <-  cm_KNN$table[1,1]
KNNcm [[r,2]] <-  cm_KNN$table[1,2]
KNNcm [[r,3]] <-  cm_KNN$table[2,1]
KNNcm [[r,4]] <-  cm_KNN$table[2,2]
err_matrix [[r,1]] <-  (cm_KNN$table[1,2]+cm_KNN$table[2,1])/nrow( data_test)
# store the errors (change the 1 to whichever model you have)
sensitivity_matrix[[r, 1]] <- cm_KNN$byClass[1]
fmeasure_matrix [[r, 1]] <- cm_KNN$byClass[7]
gmean_matrix [[r, 1]] <- sqrt(cm_KNN$byClass[1]* cm_KNN$byClass[2])
################################################################### GLM
model_glm_1 = suppressWarnings(
train(Absent_time ~ .,
data = data_train,
method = "glm",
family = 'binomial')
)
yhat_glm = predict(model_glm_1, newdata = data_test[,-20])
cm_glm = confusionMatrix(data = yhat_glm, reference = data_test[,20], positive = "1")
glmcm [[r,1]] <-  cm_glm$table[1,1]
glmcm [[r,2]] <-  cm_glm$table[1,2]
glmcm [[r,3]] <-  cm_glm$table[2,1]
glmcm [[r,4]] <-  cm_glm$table[2,2]
err_matrix [[r,2]] <-  (cm_glm$table[1,2]+cm_glm$table[2,1])/nrow( data_test)
# store the errors (change the 1 to whichever model you have)
sensitivity_matrix[[r, 2]] <- cm_glm$byClass[1]
fmeasure_matrix [[r, 2]] <- cm_glm$byClass[7]
gmean_matrix [[r, 2]] <- sqrt(cm_glm$byClass[1]* cm_glm$byClass[2])
#####################################################Decision Tree
tree_mod = rpart(Absent_time ~ ., data = data_train)
#prediction
yhat_tree = predict(tree_mod, data_test, type = 'class')
#generate confusion matrix
cm_tree <-  confusionMatrix(data = table(yhat_tree, data_test$Absent_time), reference = data_test[,-20], positive = "1")
Treecm[[r,1]] <-  cm_tree$table[1,1]
Treecm[[r,2]] <-  cm_tree$table[1,2]
Treecm[[r,3]] <-  cm_tree$table[2,1]
Treecm[[r,4]] <-  cm_tree$table[2,2]
#store the errors
err_matrix[r, 3] = mean(yhat_tree != data_test$Absent_time)
# store the errors
sensitivity_matrix[[r, 3]] <- cm_tree$byClass[1]
cm_tree$byClass[1]
fmeasure_matrix[[r, 3]] <- cm_tree$byClass[7]
gmean_matrix[[r, 3]] <- sqrt(cm_tree$byClass[1]* cm_tree$byClass[2])
#################################################### RF
rf <- randomForest(Absent_time ~.,
data=data_train,
mtry=6,
ntree=50,
na.action=na.roughfix)
yhat_rf = predict(rf, newdata = data_test, type= 'class')
cm_rf = confusionMatrix(data = yhat_rf, reference = data_test[,20], positive = "1")
rfcm [[r,1]] <-  cm_rf$table[1,1]
rfcm [[r,2]] <-  cm_rf$table[1,2]
rfcm [[r,3]] <-  cm_rf$table[2,1]
rfcm [[r,4]] <-  cm_rf$table[2,2]
err_matrix [[r,4]] <-  (cm_glm$table[1,2]+cm_glm$table[2,1])/nrow( data_test)
sensitivity_matrix[[r, 4]] <- cm_rf$byClass[1]
fmeasure_matrix[[r, 4]] <- cm_rf$byClass[7]
gmean_matrix[[r, 4]] <- sqrt(cm_rf$byClass[1]* cm_rf$byClass[2])
################################################################ SVM
#Number of C to observe
n.c = 27
#Create a sequence to try out 27 values between 2^-7 and 2^7
v.c = seq(2^(-7),2^7, length=n.c)
cv.for.c = numeric(n.c)
error.for.c = numeric(n.c)
for(j in 1:n.c) {
# loop through each value of C to try
c.svm = svm(Absent_time~., data=data_train, cross=5, C=v.c[j],
type='C-classification')
#predict and get error
pred_class <- predict(c.svm, data_test[,-20])
error.for.c[j] <- mean(data_test$Absent_time != pred_class)
}
# find the optimal C value based on cv
c.opt = v.c[min(which(cv.for.c==min(cv.for.c)))]
#based on prediction error
c.opt2 = v.c[min(which(error.for.c==min(error.for.c)))]
c.opt2
# what's the optimal C value? Use it below to re-run SVM
csvm_absent =   svm(Absent_time~., data=data_train, cross=5, C= c.opt2,
type='C-classification')
#prediction
y_hat_csvm = predict(csvm_absent, data_test[,-20])
#generate confusion matrix ( the 1 tells the model we care about that output)
cm_SVM = confusionMatrix(data = y_hat_csvm, reference = data_test[,20], positive = "1")
SVMcm [[r,1]] <-  cm_SVM$table[1,1]
SVMcm [[r,2]] <-  cm_SVM$table[1,2]
SVMcm [[r,3]] <-  cm_SVM$table[2,1]
SVMcm [[r,4]] <-  cm_SVM$table[2,2]
# store the errors (change the 1 to whichever model you have)
err_matrix[r,5] = (cm_SVM$table[1,2]+cm_SVM$table[2,1])/nrow(data_test)
sensitivity_matrix[[r, 5]] <- cm_SVM$byClass[1]
fmeasure_matrix [[r, 5]] <- cm_SVM$byClass[7]
gmean_matrix [[r, 5]] <- sqrt(cm_SVM$byClass[1]* cm_SVM$byClass[2])
#statement indicates where in loop
cat("Finished Rep",r, "\n")
}
