plot(y = fbnorm, x = q, type = 'b', xlab = 'q',
ylab = 'Squared Frobenius', main = 'Scree with FB Norm')
#set q
q <- seq(from = 2, to = 162, length = 161)
load(file = "~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/mods.RData")
load(file = "~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/fbnorm.RData")
plot(y = fbnorm, x = q, type = 'b', xlab = 'q',
ylab = 'Squared Frobenius', main = 'Scree with FB Norm')
plot(y = fbnorm[1:100], x = q[1:100], type = 'b', xlab = 'q',
ylab = 'Squared Frobenius', main = 'Scree with FB Norm')
#store the optimal nmf
optimal.nmf <- mods[[30]]
#store the optimal NMF's W
#W is the left matrix of decomposition
#the basis vectors of the lower dimension that the images have been projected to
#This is used to reconstruct faces in lower dimension
What <- optimal.nmf$W
#store the optimal NMF's latent projection
#H is the right matrix of decomposition
#together with W we can recreate X
latent.nmf <- optimal.nmf$H
#to see X, that is W%*%H, we can call optimal.nmf$fit
#code to create function for displaying images easier
rowToMatrix <- function(row){
intensity <- as.numeric(row)/max(as.numeric(row)) #RGB color intensity
return(t(matrix((rgb(intensity, intensity, intensity)), 28, 23)))
}
par(mfrow = c(3, 3))
for(i in 1:9){
#plot
display(Image(rowToMatrix(What[,i])), method = "raster", frame = 0, all = TRUE)
}
pv <- 0.90 # % variation to capture
#average per picture, aka by row
mean.mat <- matrix(rep(rowMeans(Xt), n), nrow = p, ncol = n)
#set q
q <- seq(from = 2, to = 162, length = 161)
#we want X as a p by n for rnmf so transpose it
Xt <- t(X)
pv <- 0.90 # % variation to capture
#average per picture, aka by row
mean.mat <- matrix(rep(rowMeans(Xt), n), nrow = p, ncol = n)
#PCA required standardized data
#standardize the data by subtracting the average
mean.X <- Xt - mean.mat
#PCA
pc.img <- princomp(mean.X)
#for a screeplot we can use
screeplot(pc.img)
#to accurately determine how many PCs to use to capture 90% variation:
lambda <- (pc.img$sdev)^2
plambda <- cumsum(lambda/sum(lambda))
k <- min(which(plambda >= pv))
cat(100*pv, '% of the variation gives us ', k, ' principal components\n')
#L is the V matrix
L <- pc.img$loadings
Z <- pc.img$scores
Wt <- as.matrix(Xt)%*%as.matrix(L)
#only use the # pcs, k, that gives 90% variation
#these are the eigen faces
W <- Wt[,1:k]
#latent space projections
latent.pca <- t(W)%*%mean.X
par(mfrow <- c(3,3))
for(j in 1:9){
display(t(matrix(W[,j], nrow = 28, ncol = 23)), method = "raster", frame = 0, all = TRUE)
}
# indices for each face
y = rep(1:40,each=10)
# create the matrix to store the errors
error.mat.nmf = matrix(0,ncol=2,nrow=50)
error.mat.pca = matrix(0,ncol=2,nrow=50)
for(r in 1:50){
# stratify the dataset
# the same split is used for NMF and PCA
ind = rminer::holdout(y,ratio=.6, mode='stratify')
train <- ind$tr
test <- ind$ts
#### NMF
# store the training split
tr.x.nmf = base::t(latent.nmf)[train,]
# store the test split
te.x.nmf = base::t(latent.nmf)[test,]
#### PCA
# store the training split
tr.x.pca = base::t(latent.pca)[train,]
# store the test split
te.x.pca = base::t(latent.pca)[test,]
####  Response
# store the training split
tr.y = y[train]
# store the test split
te.y = y[test]
################# KNN
knn.mod.nmf = knn(train = tr.x.nmf,
cl = as.factor(tr.y),
test = te.x.nmf,
k=1)
knn.mod.pca = knn(train=tr.x.pca,
cl=as.factor(tr.y),
test=te.x.pca,
k=1)
error.mat.nmf[r,1] = mean(knn.mod.nmf!=te.y)
error.mat.pca[r,1] = mean(knn.mod.pca!=te.y)
################# RF
rf.mod.nmf = randomForest(x=tr.x.nmf, y=as.factor(tr.y), ntree=50)
rf.mod.pca = randomForest(x=tr.x.pca, y=as.factor(tr.y), ntree=50)
error.mat.nmf[r,2] = mean(predict(rf.mod.nmf, newdata=te.x.nmf)!=te.y)
error.mat.pca[r,2] = mean(predict(rf.mod.pca, newdata=te.x.pca)!=te.y)
gc() # clear up your memory
cat("Finished rep",r,"\n")
}
# stratify the dataset
# the same split is used for NMF and PCA
ind = rminer::holdout(y,ratio=.6, mode='stratify')
#### NMF
# store the training split
tr.x.nmf = base::t(latent.nmf)[train,]
#### NMF
# store the training split
tr.x.nmf = base::t(latent.nmf)[ind$tr,]
# store the test split
te.x.nmf = base::t(latent.nmf)[ind$ts,]
# indices for each face
y <-  rep(1:10,each=10)
# indices for each face
y <-  rep(1:10,each=10)
# indices for each face
y <-  rep(1:10,each=10)
# create the matrix to store the errors
error.mat.nmf <-  matrix(0,ncol=2,nrow=50)
error.mat.pca <-  matrix(0,ncol=2,nrow=50)
for(r in 1:50){
# stratify the dataset
# the same split is used for NMF and PCA
ind = rminer::holdout(y,ratio=.6, mode='stratify')
#### NMF
# store the training split
tr.x.nmf = base::t(latent.nmf)[ind$tr,]
# store the test split
te.x.nmf = base::t(latent.nmf)[ind$ts,]
#### PCA
# store the training split
tr.x.pca = base::t(latent.pca)[train,]
# store the test split
te.x.pca = base::t(latent.pca)[test,]
####  Response
# store the training split
tr.y = y[train]
# store the test split
te.y = y[test]
################# KNN
knn.mod.nmf = knn(train = tr.x.nmf,
cl = as.factor(tr.y),
test = te.x.nmf,
k=1)
knn.mod.pca = knn(train=tr.x.pca,
cl=as.factor(tr.y),
test=te.x.pca,
k=1)
error.mat.nmf[r,1] = mean(knn.mod.nmf!=te.y)
error.mat.pca[r,1] = mean(knn.mod.pca!=te.y)
################# RF
rf.mod.nmf = randomForest(x=tr.x.nmf, y=as.factor(tr.y), ntree=50)
rf.mod.pca = randomForest(x=tr.x.pca, y=as.factor(tr.y), ntree=50)
error.mat.nmf[r,2] = mean(predict(rf.mod.nmf, newdata=te.x.nmf)!=te.y)
error.mat.pca[r,2] = mean(predict(rf.mod.pca, newdata=te.x.pca)!=te.y)
gc() # clear up your memory
cat("Finished rep",r,"\n")
}
# indices for each face
y <-  rep(1:10,each=10)
# create the matrix to store the errors
error.mat.nmf <-  matrix(0,ncol=2,nrow=50)
error.mat.pca <-  matrix(0,ncol=2,nrow=50)
for(r in 1:50){
# stratify the dataset
# the same split is used for NMF and PCA
ind = rminer::holdout(y,ratio=.6, mode='stratify')
#### NMF
# store the training split
tr.x.nmf = base::t(latent.nmf)[ind$tr,]
# store the test split
te.x.nmf = base::t(latent.nmf)[ind$ts,]
#### PCA
# store the training split
tr.x.pca = base::t(latent.pca)[ind$tr,]
# store the test split
te.x.pca = base::t(latent.pca)[ind$ts,]
####  Response
# store the training split
tr.y = y[ind$tr]
# store the test split
te.y = y[ind$ts]
################# KNN
knn.mod.nmf = knn(train = tr.x.nmf,
cl = as.factor(tr.y),
test = te.x.nmf,
k=1)
knn.mod.pca = knn(train=tr.x.pca,
cl=as.factor(tr.y),
test=te.x.pca,
k=1)
error.mat.nmf[r,1] = mean(knn.mod.nmf!=te.y)
error.mat.pca[r,1] = mean(knn.mod.pca!=te.y)
################# RF
rf.mod.nmf = randomForest(x=tr.x.nmf, y=as.factor(tr.y), ntree=50)
rf.mod.pca = randomForest(x=tr.x.pca, y=as.factor(tr.y), ntree=50)
error.mat.nmf[r,2] = mean(predict(rf.mod.nmf, newdata=te.x.nmf)!=te.y)
error.mat.pca[r,2] = mean(predict(rf.mod.pca, newdata=te.x.pca)!=te.y)
gc() # clear up your memory
cat("Finished rep",r,"\n")
}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(proto, readr, EBImage, rNMF, rminer, ggplot2, class, reshape2, stringr, randomForest)
setwd("~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/")
X <- as.matrix(read.csv("orl-faces-homework.csv",
colClasses = c('NULL', #ignore the first column
rep('numeric', times = 644))))
dim(X)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(proto, readr, EBImage, rNMF, rminer, ggplot2, class, reshape2, stringr, randomForest)
# indices for each face
y <-  rep(1:10,each=10)
# create the matrix to store the errors
error.mat.nmf <-  matrix(0,ncol=2,nrow=50)
error.mat.pca <-  matrix(0,ncol=2,nrow=50)
for(r in 1:50){
# stratify the dataset
# the same split is used for NMF and PCA
ind = rminer::holdout(y,ratio=.6, mode='stratify')
#### NMF
# store the training split
tr.x.nmf = base::t(latent.nmf)[ind$tr,]
# store the test split
te.x.nmf = base::t(latent.nmf)[ind$ts,]
#### PCA
# store the training split
tr.x.pca = base::t(latent.pca)[ind$tr,]
# store the test split
te.x.pca = base::t(latent.pca)[ind$ts,]
####  Response
# store the training split
tr.y = y[ind$tr]
# store the test split
te.y = y[ind$ts]
################# KNN
knn.mod.nmf = knn(train = tr.x.nmf,
cl = as.factor(tr.y),
test = te.x.nmf,
k=1)
knn.mod.pca = knn(train=tr.x.pca,
cl=as.factor(tr.y),
test=te.x.pca,
k=1)
error.mat.nmf[r,1] = mean(knn.mod.nmf!=te.y)
error.mat.pca[r,1] = mean(knn.mod.pca!=te.y)
################# RF
rf.mod.nmf = randomForest(x=tr.x.nmf, y=as.factor(tr.y), ntree=50)
rf.mod.pca = randomForest(x=tr.x.pca, y=as.factor(tr.y), ntree=50)
error.mat.nmf[r,2] = mean(predict(rf.mod.nmf, newdata=te.x.nmf)!=te.y)
error.mat.pca[r,2] = mean(predict(rf.mod.pca, newdata=te.x.pca)!=te.y)
gc() # clear up your memory
cat("Finished rep",r,"\n")
}
colnames(fin.error.mat.pca) <- c("KNN-PCA", "FR-PCA")
#name the cols with method and projection type
fin.error.mat.nmf <- as.data.frame(error.mat.nmf)
colnames(fin.error.mat.nmf) <- c("KNN-NMF", "RF-NMF")
fin.error.mat.pca <- as.data.frame(error.mat.pca)
colnames(fin.error.mat.pca) <- c("KNN-PCA", "FR-PCA")
#combine
fin.error.mat <- as.data.frame(cbind(fin.error.mat.nmf, fin.error.mat.pca))
#melt and prepare for ggplot
melted.final <- melt(fin.error.mat)
colnames(melted.final) <- c("Method", "Error")
#grab the projection and classifier info
melted.final$Projection <- str_sub(melted.final$Method, -3, -1)
melted.final$Classifier <- str_sub(melted.final$Method, 1, -5)
save("~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/melted.final.RData")
save(melted.final, file = "~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/melted.final.RData")
load(file = "~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/melted.final.RData")
#boxplot
ggplot(data = melted.final,
mapping = aes(x = Projection, y = Error, fill = Projection)) +
facet_wrap(~Classifier) +
goem_boxplot()
#boxplot
ggplot(data = melted.final,
mapping = aes(x = Projection, y = Error, fill = Projection)) +
facet_wrap(~Classifier) +
geom_boxplot()
#name the cols with method and projection type
fin.error.mat.nmf <- as.data.frame(error.mat.nmf)
colnames(fin.error.mat.nmf) <- c("KNN-NMF", "RF-NMF")
fin.error.mat.pca <- as.data.frame(error.mat.pca)
colnames(fin.error.mat.pca) <- c("KNN-PCA", "RF-PCA")
#combine
fin.error.mat <- as.data.frame(cbind(fin.error.mat.nmf, fin.error.mat.pca))
#melt and prepare for ggplot
melted.final <- melt(fin.error.mat)
colnames(melted.final) <- c("Method", "Error")
#grab the projection and classifier info
melted.final$Projection <- str_sub(melted.final$Method, -3, -1)
melted.final$Classifier <- str_sub(melted.final$Method, 1, -5)
save(melted.final, file = "~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/melted.final.RData")
load(file = "~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/melted.final.RData")
#boxplot
ggplot(data = melted.final,
mapping = aes(x = Projection, y = Error, fill = Projection)) +
facet_wrap(~Classifier) +
geom_boxplot()
if (!require("pacman")) install.packages("pacman")
pacman::p_load(proto, readr, EBImage, rNMF, rminer, ggplot2, class, reshape2, stringr, randomForest)
setwd("~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/")
X <- as.matrix(read.csv("orl-faces-homework.csv",
colClasses = c('NULL', #ignore the first column
rep('numeric', times = 644))))
dim(X)
#Set values for n, p, r, c based on the data
#r and c refer to the number of rows and columns per photo
n = nrow(X)
p = ncol(X)
r = 28
c = 23
#set q
q <- seq(from = 2, to = 162, length = 161)
#we want X as a p by n for rnmf so transpose it
Xt <- t(X)
load(file = "~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/mods.RData")
load(file = "~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/fbnorm.RData")
plot(y = fbnorm, x = q, type = 'b', xlab = 'q',
ylab = 'Squared Frobenius', main = 'Scree with FB Norm')
plot(y = fbnorm[1:100], x = q[1:100], type = 'b', xlab = 'q',
ylab = 'Squared Frobenius', main = 'Scree with FB Norm')
#store the optimal nmf
optimal.nmf <- mods[[30]]
#store the optimal NMF's W
#W is the left matrix of decomposition
#the basis vectors of the lower dimension that the images have been projected to
#This is used to reconstruct faces in lower dimension
What <- optimal.nmf$W
#store the optimal NMF's latent projection
#H is the right matrix of decomposition
#together with W we can recreate X
latent.nmf <- optimal.nmf$H
#to see X, that is W%*%H, we can call optimal.nmf$fit
#code to create function for displaying images easier
rowToMatrix <- function(row){
intensity <- as.numeric(row)/max(as.numeric(row)) #RGB color intensity
return(t(matrix((rgb(intensity, intensity, intensity)), 28, 23)))
}
par(mfrow = c(3, 3))
for(i in 1:9){
#plot
display(Image(rowToMatrix(What[,i])), method = "raster", frame = 0, all = TRUE)
}
pv <- 0.90 # % variation to capture
#average per picture, aka by row
mean.mat <- matrix(rep(rowMeans(Xt), n), nrow = p, ncol = n)
#PCA required standardized data
#standardize the data by subtracting the average
mean.X <- Xt - mean.mat
#PCA
pc.img <- princomp(mean.X)
#for a screeplot we can use
screeplot(pc.img)
#to accurately determine how many PCs to use to capture 90% variation:
lambda <- (pc.img$sdev)^2
plambda <- cumsum(lambda/sum(lambda))
k <- min(which(plambda >= pv))
cat(100*pv, '% of the variation gives us ', k, ' principal components\n')
#L is the V matrix
L <- pc.img$loadings
Z <- pc.img$scores
Wt <- as.matrix(Xt)%*%as.matrix(L)
#only use the # pcs, k, that gives 90% variation
#these are the eigen faces
W <- Wt[,1:k]
#latent space projections
latent.pca <- t(W)%*%mean.X
par(mfrow <- c(3,3))
for(j in 1:9){
display(t(matrix(W[,j], nrow = 28, ncol = 23)), method = "raster", frame = 0, all = TRUE)
}
# indices for each face
y <-  rep(1:10,each=10)
# create the matrix to store the errors
error.mat.nmf <-  matrix(0,ncol=2,nrow=50)
error.mat.pca <-  matrix(0,ncol=2,nrow=50)
for(r in 1:50){
# stratify the dataset
# the same split is used for NMF and PCA
ind = rminer::holdout(y,ratio=.6, mode='stratify')
#### NMF
# store the training split
tr.x.nmf = base::t(latent.nmf)[ind$tr,]
# store the test split
te.x.nmf = base::t(latent.nmf)[ind$ts,]
#### PCA
# store the training split
tr.x.pca = base::t(latent.pca)[ind$tr,]
# store the test split
te.x.pca = base::t(latent.pca)[ind$ts,]
####  Response
# store the training split
tr.y = y[ind$tr]
# store the test split
te.y = y[ind$ts]
################# KNN
knn.mod.nmf = knn(train = tr.x.nmf,
cl = as.factor(tr.y),
test = te.x.nmf,
k=1)
knn.mod.pca = knn(train=tr.x.pca,
cl=as.factor(tr.y),
test=te.x.pca,
k=1)
error.mat.nmf[r,1] = mean(knn.mod.nmf!=te.y)
error.mat.pca[r,1] = mean(knn.mod.pca!=te.y)
################# RF
rf.mod.nmf = randomForest(x=tr.x.nmf, y=as.factor(tr.y), ntree=50)
rf.mod.pca = randomForest(x=tr.x.pca, y=as.factor(tr.y), ntree=50)
error.mat.nmf[r,2] = mean(predict(rf.mod.nmf, newdata=te.x.nmf)!=te.y)
error.mat.pca[r,2] = mean(predict(rf.mod.pca, newdata=te.x.pca)!=te.y)
gc() # clear up your memory
# cat("Finished rep",r,"\n")
}
load(file = "~/Documents/GitHub/Stat-Learning-2018-Fall/Week 6 - Dimensionality Reduction in Images/Week_6_Data/Week6_SarahBaylor/melted.final.RData")
#boxplot
ggplot(data = melted.final,
mapping = aes(x = Projection, y = Error, fill = Projection)) +
facet_wrap(~Classifier) +
geom_boxplot()
dat <- read.table("~/Documents/Notre Dame/Behavioral Data Science/Week 7/week07-practice.txt")
str(dat)
dat <- read.delim("~/Documents/Notre Dame/Behavioral Data Science/Week 7/week07-practice.txt",
header = TRUE,
sep = "|")
str(dat)
install.packages("mice")
install.packages("MissMech")
#load the necessary packages
library(tidyverse)
library(psych)
library(lme4)
library(ggplot2)
library(mice)
library(MissMech)
library(magrittr)
dat <- read.delim("~/Documents/Notre Dame/Behavioral Data Science/Week 7/week07-practice.txt",
header = TRUE,
sep = "|")
str(dat)
```{r}
#create the factors used in week 4
obliqueRotation <- fa(r = dat[, 4:10], nfactors = 2, rotate = "promax")
#add the two factor labels from week 4 to the data set
dat2 <- dat %>%
mutate(score = obliqueRotation$scores[, "MR1"],
personal = obliqueRotation$scores[, "MR2"])
#view the new data set
glimpse(dat2)
#view a summary of the new data set
summary(dat2)
mcarTest <- TestMCARNormality(dat[,4:14])
mcarTest <- TestMCARNormality(dat[ , 4:14])
mcarTest <- TestMCARNormality(dat[ , 4:13])
mcarTest <- TestMCARNormality(dat[ , 4:10])
mcarTest
imputedData <- mice(dat, m = 10, maxit = 20,
method = "cart",
pred = quickpred(dat, minpuc = 0.2, mincor = 0.01),
print = FALSE)
ptm = proc.time()
imputedData <- mice(dat, m = 10, maxit = 20,
method = "cart",
pred = quickpred(dat, minpuc = 0.2, mincor = 0.01),
print = FALSE)
proc.time() - ptm
imputedData <- mice(dat, maxit = 0)
imputedData
plot(imputedData)
plot(imputedData, layout = c(2,1))
plot(imputedData)
plot(imputedData)
imputedData2 <- mice(dat, m = 10, maxit = 20,
method = c("cart", "cart", "cart",
"cart", "cart", "pmm"),
pred = quickpred(dat, minpuc = .2, mincor = .01),
print = FALSE)
imputedData2 <- mice(dat, m = 10, maxit = 20,
method = c("cart", "cart", "cart",
"cart", "cart", "pmm"),
pred = quickpred(dat, minpuc = .2, mincor = .01),
print = FALSE)
plot(imputedData)
plot(imputedData)
a)
imputedData2 <- mice(dat, m = 10, maxit = 20,
method = c("cart", "cart", "cart",
"cart", "pmm"),
pred = quickpred(dat, minpuc = .2, mincor = .01),
print = FALSE)
imputedData2 <- mice(dat, m = 10, maxit = 20,
method = c("cart", "cart", "cart", "pmm"),
pred = quickpred(dat, minpuc = .2, mincor = .01),
print = FALSE)
imputedData2 <- mice(dat, m = 10, maxit = 20,
method = "pmm",
pred = quickpred(dat, minpuc = .2, mincor = .01),
print = FALSE)
