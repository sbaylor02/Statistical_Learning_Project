####################
STOP; RUN ALL CODE FROM THROUGH After the EDA Response Variable SECTION PRIOR TO CODING FURTHER!!!!!!!!!!!!!!!!!!!!
#####################


---
title: "Statistical Learning Project"
output: html_notebook
---

Sarah, Pavel, Rose, Catherine, Shravya
East
Statistical Learning Project


```{r}
library(tidyverse)
library(reshape2)
library(readxl)
library(caret)
library(rpart) 
library(partykit) 
library(randomForest)
library(class)
library (rminer)
```

```{r}

#Read in the data
dat <- read_excel("Absenteeism_at_work.xls")

#View the data
glimpse(dat)

```
##Pre-Processing Data
```{r}
#Set factored variables as factors
col <- c("ID", "Reason for absence", "Month of absence", "Day of the week", "Seasons", "Disciplinary failure", "Education", "Social drinker",   "Social smoker", "Pet", "Son")
dat[col] <- lapply(dat[col], as.factor)

#Rename the columns for easier use
colnames(dat) <- c("ID", "Reason", "Month", "Day", "Seasons", "Transportation_expense", "Distance", "Service_time", "Age", "Work_load", "Hit_target", "Disciplinary_failure", "Education", "Children", "Social_drinker", "Social_smoker", "Pet", "Weight", "Height", "BMI", "Absent_time")

#View the data
glimpse(dat)



```


```{r}
# Still need to reset levels on vrariables so ordered factors are graphed in order
```

```{r}
nums <- unlist(lapply(dat, is.numeric))  
dat.num <- dat[ , nums]

```
## EDA Response Variable

### Absent_time
```{r}
summary(dat$Absent_time)

```


```{r}
table(dat$Absent_time)
```

```{r}
#change variable represent missed time one day or greater
dat <- dat %>% mutate(Absent_time= ifelse(dat$Absent_time <=8,0,1))
```

```{r}
ggplot(data = dat,
       aes(x = Absent_time)) +
  geom_histogram() + 
  theme_minimal()
```
```{r}
dat %>%
  gather(-Absent_time, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Absent_time)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
```


## EDA Predictors

### ID
### Reason
### Month
### Day

```{r}
dat %>%
  gather(-Day, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Day)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
```


### Seasons

```{r}
#Scatterplots for variable 'Seasons'
dat %>%
  gather(-Seasons, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Seasons)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
```
### Transportation Expense

```{r}
summary(dat$Transportation_expense)
ggplot(data = dat,
       aes(x = Transportation_expense)) +
  geom_histogram(binwidth = 50) + 
  theme_minimal()
```

```{r}
dat %>%
  gather(-Transportation_expense, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Transportation_expense)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
# Possible positive correlation seen between distance and Transportation_expense
```

###Distance
```{r}
summary(dat$Distance)
ggplot(data = dat,
       aes(x = Distance)) +
  geom_histogram(binwidth = 5) + 
  theme_minimal()
```

```{r}
dat %>%
  gather(-Distance, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Distance)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
#Possible Positive correlation seen between distance and Transportation_expense
```

### Service Time
```{r}
ggplot(data = dat,
       aes(x = Service_time)) +
  geom_histogram() +
  theme_minimal()
```

```{r}
dat %>%
  gather(-Service_time, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Service_time)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")

```

### Age

```{r}
ggplot(data = dat,
       aes(x = Age)) +
  geom_histogram() + 
  theme_minimal()
```

```{r}
dat %>%
  gather(-Age, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Age)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")

```
### Workload

```{r}
summary(dat$Work_load)

ggplot(data = dat,
       aes(x = Work_load)) +
  geom_histogram(binwidth = 5000) + 
  theme_minimal()
```

```{r}
dat %>%
  gather(-Work_load, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Work_load)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
```

### Hit Target

```{r}
ggplot(data = dat,
       aes(x = Hit_target)) + 
  geom_histogram() +
  theme_minimal()
```

```{r}
dat %>%
  gather(-Hit_target, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Hit_target)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")

```
### Disciplinary Failure
dat %>%
  gather(-Disciplinary_failure, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Disciplinary_failure)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
  
### Education

dat %>%
  gather(-Education, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Education)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
  
### Children

dat %>%
  gather(-Children, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Children)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
### Social Drinker

dat %>%
  gather(-Social_drinker, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Social_drinker)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
  
###Social Smoker

```{r}
#Scatterplots for variable 'Social_smoker'
dat %>%
  gather(-Social_smoker, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Social_smoker)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
```


### Pet
### Weight

```{r}
dat %>%
  gather(-Weight, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Weight)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
```
### Height

dat.num %>%
  gather(-Height, key = "var_name", value = "value") %>%
  ggplot(aes(x = value, y = Height)) +
  geom_point() +
  facet_wrap(~ var_name, scales = "free")
### BMI

## Initial Method Testing


```{r}

R <- 50 # replications

# create the matrix to store values 1 row per model
err_matrix <- matrix(0, ncol=5, nrow=R)

sensitivity_matrix <- matrix(0, ncol=5, nrow=R)

fmeasure_matrix <- matrix(0, ncol=5, nrow=R)

gmean_matrix <- matrix(0, ncol=5, nrow=R)

KNNcm <- matrix(0, ncol=4, nrow=R)

set.seed(1876)
 

for (r in 1:R){
  
# subsetting data to training and testing data
p <- .6 # proportion of data for training
w <- sample(1:nrow(dat), nrow(dat)*p, replace=F)
data_train <-dat[w,] 
data_test <- dat[-w,]

  ########### knn

#Running the classifier

  knn <- knn(data_train[-21],
                       test = data_test[-21],
                       cl=data_train$Absent_time, k=2)
 knntable <- table(data_test$Absent_time, knn)
 
#generate confusion matrix
 cm_KNN <-  confusionMatrix(data = knntable, reference = data_test[,-21], positive = "1")
 
 KNNcm [[r,1]] <-  cm_KNN$table[1,1]
 KNNcm [[r,2]] <-  cm_KNN$table[1,2]
 KNNcm [[r,3]] <-  cm_KNN$table[2,1]
 KNNcm [[r,4]] <-  cm_KNN$table[2,2]
 
 err_matrix [[r,1]] <-  (cm_KNN$table[1,2]+cm_KNN$table[2,1])/nrow( data_test)
  
  # store the errors (change the 1 to whichever model you have)   
  
 sensitivity_matrix[[r, 1]] <- cm_KNN$byClass[1]
 
 cm_KNN$byClass[1]
 
 fmeasure_matrix [[r, 1]] <- cm_KNN$byClass[7]
 
 gmean_matrix [[r, 1]] <- sqrt(cm_KNN$byClass[1]* cm_KNN$byClass[2])
  
  ############# Other models go here because they need same training and testing datasets for comparison
  
  
#statement indicates where in loop
  cat("Finished Rep",r, "\n")
}

```
Change the matrix names to make easier to interpret

```{r}
#plug in your model in the right place

colnames(err_matrix) <- c("KNN","other", "other","other", 'other')

colnames(sensitivity_matrix)<- c("KNN","other", "other","other", 'other')

colnames(fmeasure_matrix) <- c("KNN","other", "other","other", 'other')

colnames(gmean_matrix) <- c("KNN","other", "other","other", 'other')

colnames(KNNcm) <- c("True Negative","False Positive", "False Negative","True Positive" )
```

#Logistic regression - Pavel
#I ran 2 models, a regular logistic and a boosted logistic regression. Results below:

#Transforming to Data Frame
```{r}
dat_1 <- as.data.frame(dat)
```

#Transforming variable 'Absent_time' into binomial:
```{r}
dat_1$Absent_time <- ifelse(dat_1$Absent_time <= 8, 'good', 'bad')

#Examining variable 'Absent_time'
table(dat_1$Absent_time)
```

#eliminating columns 1 through 5 since they do not provide any valuable information for prediction, 
#these are just variables that describe the data:
```{r}
dat_1 <- dat_1[,-c(1:5)]
dat_1$Absent_time <- as.factor(dat_1$Absent_time)
dat_1[1:15] <- scale(dat_1[1:15])

```
```{r}
#Setting number of repetitions
R = 50

# Creating  matrix to store the errors

test_error_A = matrix(0,ncol=1,nrow=R)
test_error_B = matrix(0,ncol=1,nrow=R)
```

```{r}
#A. Running regular logistic regression
set.seed(7359)
for (r in 1:R){
  
  ind = holdout(dat_1$Absent_time,ratio=3/5, mode='stratify') 
  
  tr_xy_A = dat_1[ind$tr,] # train set
  te_XY_A = dat_1[ind$ts,] # test set
  
 # running the actual model
  model_A = train(Absent_time ~ .,
                  data = tr_xy_A,
                  method = "glm", 
                  family = 'binomial') 
  
  yhat_A = predict(model_A, newdata = te_XY_A[,-16])
  
  cm_tr = (confusionMatrix(data = yhat_A, reference = te_XY_A[,16]))
  test_error_A[[r]] = sum(yhat_A!= te_XY_A[,16])/length( te_XY_A[,16])
}
```

```{r}
#B. Boosted logistic regression.
set.seed(7359)
for (r in 1:R){ # replication loop
  
  ind = holdout(dat_1$Absent_time,ratio=3/5, mode='stratify') # stratified train/test split
  
  tr_xy_B = dat_1[ind$tr,] # train set
  te_XY_B = dat_1[ind$ts,] # test set
  
  # running the actual model
  model_B = train(Absent_time ~ .,
                  data = tr_xy_B,
                  method = "LogitBoost")
  
  yhat_B = predict(model_B, newdata = te_XY_B[,-16])
  cm_tr_B = (confusionMatrix(data = yhat_B, reference = te_XY_B[,16]))
  test_error_B[[r]] = sum(yhat_B!= te_XY_B[,16])/length( te_XY_B[,16])  
}
```

#Metrics For regular and boosted logistic regression:
```{r}
#Sensitivity:
Sensitivity_A <- cm_tr$table[1,1]/(cm_tr$table[1,1]+cm_tr$table[2,1])

Sensitivity_B <- cm_tr_B$table[1,1]/(cm_tr_B$table[1,1]+cm_tr_B$table[2,1])

Sensitivity_metric <- as.data.frame(cbind(Sensitivity_A, Sensitivity_B)) %>% 
  `rownames<-`('Sensitivity')

#Renaming the columns:
colnames(Sensitivity_metric)[c(1,2)] <- c("Regular Logistic", "Boosted Logistic")

#Displaying Sensitivity
Sensitivity_metric
```

```{r}
#Computing F1:
#F1 for Regular logistic regression:

cm_tr1 <- as.matrix(cm_tr)
nc <- nrow(cm_tr1) # number of classes
diag <- diag(cm_tr1) # number of correctly classified instances per class 
rowsums <- apply(cm_tr1, 1, sum) # number of instances per class
colsums <- apply(cm_tr1, 2, sum) # number of predictions per class

precision <- diag / colsums 
recall <- diag / rowsums 

#The F-measure gives the harmonic mean of recall and precision 
f1_A <- 2 * precision * recall / (precision + recall) 

#F1 for Boosted Logistic Regression:

cm_tr2 <- as.matrix(cm_tr_B)

nc2 = nrow(cm_tr2) # number of classes
diag2 = diag(cm_tr2) # number of correctly classified instances per class 
rowsums2 = apply(cm_tr2, 1, sum) # number of instances per class
colsums2 = apply(cm_tr2, 2, sum) # number of predictions per class

precision2 = diag2 / colsums2 
recall2 = diag2 / rowsums2 

#The F-measure gives the harmonic mean of recall and precision 
f1_B = 2 * precision2 * recall2 / (precision2 + recall2) 

#Displaying F1 for both models:
data.frame(f1_A, f1_B)

```
```{r}
#Mean of errors:

mean_of_errors <- data.frame(mean(test_error_A), mean(test_error_B))
colnames(mean_of_errors)[c(1,2)] <- c("Regular Logistic", "Boosted Logistic")
mean_of_errors
```

```{r}
#Binding data for the ggplot:
box_matrix <- cbind(test_error_A, test_error_B)
colnames(box_matrix) <- c("test_error_A","test_error_B")
box_matrix <- melt(box_matrix)

#Displaying Box Plot for errors:
ggplot(data = data.frame(box_matrix), aes(x=Var2, y=value)) +
  geom_boxplot() + theme_bw() 
```



#divide data into test and training 
#Transforming to Data Frame

dat_1 <- as.data.frame(dat)
#Transforming variable 'Absent_time' into binomial:

dat_1$Absent_time <- ifelse(dat_1$Absent_time <= 8, 'good', 'bad')

#Examining variable 'Absent_time'
table(dat_1$Absent_time)
#eliminating columns 1 through 5 since they do not provide any valuable information for prediction, #these are just variables that describe the data:

dat_1 <- dat_1[,-c(1:5)]
dat_1$Absent_time <- as.factor(dat_1$Absent_time)
dat_1[1:15] <- scale(dat_1[1:15])


set.seed(1874)
n <- nrow(dat_1)
#create an interger sample of the number of rows in the data set.
#We want 20% in the test data set so
# .2 time the numer of rows (n) gives the sample size.
# test_idx will have 20% of the row numbers in it
test_idx <- sample.int(n,size= round(0.2 * n))
#Use test_idx to subset NHANES for the test data
test <- dat_1[test_idx,]
#Use the inverse of test_idx, NOT test.idx to subset NHANES for the train data set
train <- dat_1[-test_idx,]

#Set the seed to ensure similar "random" selections as the other processes
set.seed(1874)
library(randomForest)
nam <- names(dat_1[1:15])
formfull <- as.formula(paste("Absent_time ~" , paste(nam,collapse =" + ") ) )
#Use the randomForest on the training data with 6 variables tried at each split and 100
#trees created
diag_forest <- randomForest(formfull,
                            data=train,
                            mtry=6,
                            ntree=501,
                            na.action=na.roughfix)
diag_forest
#Give the importance chart to determine which variables were most often used over the
#course of the bootstrapped forest
importance(diag_forest)

#Use the random forest to make predictions on the classification of the test data
test_forest <- predict(diag_forest,newdata = test,type="class")
#Confusion Matrix for the Random Forest with all of the variables
cm_tr<- table(test_forest,test$Absent_time)



#Computing F1:

cm_tr1 <- as.matrix(cm_tr)
cm_tr1
nc = nrow(cm_tr1) # number of classes
diag = diag(cm_tr1) # number of correctly classified instances per class 
rowsums = apply(cm_tr1, 1, sum) # number of instances per class
colsums = apply(cm_tr1, 2, sum) # number of predictions per class

precision = diag / colsums 
recall = diag / rowsums

#The F-measure gives the harmonic mean of recall and precision 
f1_rf = 2 * precision * recall / (precision + recall) 

data.frame(precision, recall, f1_rf) 


#Sensitivity:
Sensitivity_RF <- cm_tr[1,1]/(cm_tr[1,1]+cm_tr[2,1])
Sensitivity_metric <- as.data.frame(cbind(Sensitivity_RF)) %>% 
  `rownames<-`('Sensitivity')


#Renaming the columns:
colnames(Sensitivity_metric)[c(1)] <- c("Random Forest")

#Displaying Sensitivity
Sensitivity_metric



#calculating error
test_error_RF<- diag_forest$err.rate

mean_of_errors_RF <- data.frame(mean(test_error_RF))
colnames(mean_of_errors_RF)[c(1)] <- c("Random Forest")
mean_of_errors_RF

test_error_RF2 <- data.frame(test_error_RF)
test_error_RF2
#Displaying Box Plot for errors:
ggplot(test_error_RF2, aes(y=OOB)) + geom_boxplot() + labs(x= 'Random Forest', y= 'Out of Bag Error', title= 'Out of Bag Error for Random Forest')
